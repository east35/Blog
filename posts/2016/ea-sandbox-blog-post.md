---
title: Designing EA Sandbox
date: 2016-05-09
description: A Feedback Platform for Mobile Game Testing
category: Case Study
company: Funsize
company_url: https://funsize.co
---

## The Challenge

In 2016, I worked with EA Games on a unique problem: how do you efficiently gather meaningful feedback from playtesters during the critical pre-launch phase of mobile games? Traditional methods—surveys sent via email, bug reports filed in separate tools—created friction and resulted in lower response rates and less contextual feedback.

The solution was **EA Sandbox**, an app-within-an-app that embedded feedback collection directly into the playtesting experience.

![](/images/2016/sandbox/us.JPG)

## The Vision

The director I collaborated with established a clear vision: create a lightweight platform where playtesters could access pre-release EA mobile games and provide feedback without leaving the experience. The app needed to:

- Showcase multiple games in various stages of development
- Set proper expectations about pre-alpha/beta quality
- Capture both structured feedback (surveys) and unstructured feedback (chat)
- Make providing feedback feel rewarding, not burdensome

My role was to take this vision and execute every screen, interaction, and flow.

## Key Features & Design Decisions

### 1. The Game Hub

The Sandbox and Live Games tabs gave players a central place to discover testable games. Each game card clearly indicated its status (Pre-Soft Launch, Featured) and showed engaging key art. This visual-first approach helped players quickly identify games they wanted to test.

![details](</images/2016/sandbox/details 3.png>)

[grid]
![](/images/2016/sandbox/grid 3.png)
![](/images/2016/sandbox/grid 2.png)
![](/images/2016/sandbox/grid 1.png)
![](/images/2016/sandbox/grid 7.png)
[/grid]


### 2. Setting Expectations

One of the most important screens was "What to Expect" - a transparent list of known issues before players even started. This served two purposes:

- It prevented redundant bug reports about known issues
- It framed the experience correctly, turning potential frustration into collaborative problem-solving

[grid]
![](/images/2016/sandbox/grid 4.png)
![](/images/2016/sandbox/grid 5.png)
[/grid]

### 3. Contextual Feedback Collection

Rather than waiting until the end of a play session, we could trigger feedback requests at specific moments:

- **Targeted surveys** asking about specific features ("What about the green ghosts on level 2?")
- **Visual surveys** using actual game assets to ask about difficulty or character design
- **Special Attention** prompts highlighting areas where developers specifically needed input

[grid]
![](/images/2016/sandbox/grid 8.png)
![](/images/2016/sandbox/grid 9.png)
[/grid]

### 4. The Evolution Tab

This was a transparency play. Players could see previous build versions, read sandbox findings from other testers, and track how their feedback was being implemented. It created a sense of collaboration and showed players their input mattered.

### 5. Sandbot Chat

An AI-assisted chat interface made giving unstructured feedback feel conversational rather than like filling out a form. The bot could congratulate players on milestones, ask follow-up questions, and maintain context across the conversation.

![details](</images/2016/sandbox/grid 6.png>)

### 6. Persistent Reminders

The feedback alert cards used playful game characters (like the PVZ zombie) to re-engage players who hadn't completed surveys. These needed to be persistent enough to be effective but not so aggressive they became annoying.

## Execution Challenges

Every screen needed to work across:

- Different game genres (RPG, puzzle, casual)
- Multiple stages of development (pre-alpha through soft launch)
- Various feedback types (qualitative chat, quantitative surveys, visual selection)

The design system needed to be flexible enough to accommodate EA's diverse game portfolio while maintaining a consistent Sandbox brand that didn't compete with or diminish the individual game aesthetics.

## Impact

EA Sandbox provided a dedicated channel for pre-launch feedback that was:

- **More contextual** - players gave feedback while the experience was fresh
- **Higher quality** - structured surveys got specific answers to specific questions
- **More transparent** - players could see how their feedback influenced development

The app represented a shift from treating playtesters as subjects to treating them as collaborators in the development process.

## Reflection

This project taught me a lot about designing for engagement in contexts where the primary action (playing games) and the secondary action (providing feedback) are potentially at odds. The key was making feedback feel like a natural extension of the play experience rather than an interruption.

![details](</images/2016/sandbox/details 1.png>)
![details](</images/2016/sandbox/details 2.png>)
